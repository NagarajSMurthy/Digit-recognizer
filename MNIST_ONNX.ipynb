{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvMNIST, self).__init__()  # Indicates that the ConvMNIST class inherits properties from nn.Module.\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,32,3,1,1)      # 28*28*3\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1,1) \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2) # 14*14*32\n",
    "        self.fc2 = nn.Linear(64*14*14,1000)\n",
    "        self.fc3 = nn.Linear(1000,256)\n",
    "        self.fc4 = nn.Linear(256,10)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1,64*14*14)\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onnx_model = ConvMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the saved models\n",
    "os.listdir('mnist_model_new/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the saved models\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and instantiate the model\n",
    "onnx_model = torch.load(path,map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to eval to ensure normal operation of dropouts.\n",
    "onnx_model.eval()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We first trace our model to export it to the onnx format. This is done by passing a dummy variable (of proper size) as input to the model. The onnx model also stores the learned parameters and the various operations that contribute to the output. The input size will be fixed in the exported ONNX graph for all the input dimensions. If the model has variable parameters like batch_size, we can specify that as dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the model name to be saved in onnx format\n",
    "onnx_model_name = 'MnistConv.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1 # set as a random number initially (it is set as dynamic axes later)\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 1, 28, 28, requires_grad=True) \n",
    "torch_out = onnx_model(x)    # storing the model output to compare with the onnx model output\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(onnx_model,                # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  onnx_model_name,      # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes \n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_onnx = onnx.load(onnx_model_name)\n",
    "onnx.checker.check_model(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnx_model_name)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().numpy() if tensor.requires_grad else tensor.numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "# ort_outs gives a list of outputs. We are selecting the first one, since we have one output.\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result is valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('handwritten-digits-images-dataset/handwritten-digits-images-dataset/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the test data\n",
    "root = 'handwritten-digits-images-dataset/handwritten-digits-images-dataset/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Grayscale(),\n",
    "                                transforms.ToTensor()\n",
    "                                #transforms.Normalize((0.5,), (0.5,)),\n",
    "                               ])\n",
    "\n",
    "\n",
    "testset = datasets.ImageFolder(root+'test', transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Passing a single image\n",
    "# Since we have set dynamic axes for batch size, it tells the onnxruntime that batch_size can vary.\n",
    "# We can pass any number of images.\n",
    "data = iter(testloader)\n",
    "img, label = data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#img = img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For gray scale image inputs\n",
    "img = img[5].unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gray scale image plot\n",
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img)}     # The input must be a numpy array \n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "predicted = ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a =list(np.exp(predicted).squeeze())\n",
    "index = a.index(max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted class {index}\")\n",
    "print(label[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
